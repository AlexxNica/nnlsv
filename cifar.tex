% Template for PLoS
% Version 1.0 January 2009
%
% To compile to pdf, run:
% latex plos.template
% bibtex plos.template
% latex plos.template
% latex plos.template
% dvipdf plos.template

\documentclass[10pt]{article}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{color} 

% Use doublespacing - comment out for single spacing
\usepackage{setspace} 
\doublespacing


% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

% Use the PLoS provided bibtex style
\bibliographystyle{plos2009}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother


% Leave date blank
\date{}

\pagestyle{myheadings}
%% ** EDIT HERE **


%% ** EDIT HERE **
%% PLEASE INCLUDE ALL MACROS BELOW

%% END MACROS SECTION

\begin{document}

% Title must be 150 characters or less
\begin{flushleft}
{\Large
\textbf{Title}
}
%
% XXX
%
% VISUALIZING THE SPACE OF HYPERPARAMETERS IN NEURAL NETWORKS
% 
% LARGE-SCALE NETWORK TRAINING FOR EMPIRICAL ANALYSIS OF NEURAL NET PARAMETERS
%
% VISUALIZING HOW NEURAL NETWORKS LEARN
%
% VISUALIZING THE ENTIRE SPACE OF NEURAL NETWORK HYPER-PARAMETERS
%
% VISUSALIZING HOW NEURAL NETWORKS LEARN THROUGH LARGE-SCALE NETWORK TRAINING
%
% VISUALIZING HOW NEURAL NETWORK HYPER-PARAMETERS CHANGE LEARNING
%
% VISUALIZING HOW HYPER-PARAMETERS AFFECT LEARNING IN NEURAL NETWORKS
%
% Insert Author names, affiliations and corresponding author email.
\\
Christopher Olah$^{1,\ast}$, 

Michael A. Nielsen$^{2}$
\\
\bf{1} Hacklab Toronto, 170a Baldwin Street, Toronto, Ontario, Canada
\\
\bf{2} 2/120 Christie Street, Toronto, Ontario, Canada
\\
$\ast$ E-mail: colah@thielfellowship.org

\end{flushleft}

% Please keep the abstract between 250 and 300 words
\section*{Abstract}

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLoS ONE authors please skip this step. 
% Author Summary not valid for PLoS ONE submissions.   
\section*{Author Summary}

\section*{Introduction}

%
% the trouble with neural nets
%
Artificial neural networks are a powerful machine learning technique
which achieve excellent results for many pattern recognition problems
(e.g. KSH, Ng, Seide, Dahl, Schmidhuber, Bengio on Maxout).  But while
powerful, they can be difficult to train, and performance often
depends sensitively on the choice of hyper-parameters for the neural
network.  Picking these hyper-parameters can be an art-form, often
involving considerable trial and error based on hard-to-describe
heuristics. (XXX - It'd be nice to have refs). Casual users of neural
networks sometimes get poor results for a problem they expect to be
soluble by their network.  At that point, they face a conundrum: are
the poor results because of some easily-fixed issue with the
hyper-parameters, or is there some deeper obstacle?  Because of these
concerns some people advocate sticking with less finicky methods such
as support vector machines and random forests.

%
% what we do in this paper: broad picture
%
In this paper, we use clusters of graphics processing units (GPUs) to
train more than 100,000 neural nets.  This lets us systematically
explore hyper-parameter space, and to develop vivid visualizations and
animations showing how the network learns for different choices of
hyper-parameters.  From these visualizations we extract several
heuristic rules of thumb for choosing neural network hyper-parameters.

%
% the flavour
%
To convey the type of experiments we do and the results we obtain,
consider the graph in Figure~\label{fig:example}.  This graph shows
the performance of a single neural network architecture on a
particular image recognition task.  The important thing is that
performance is shown for a very large number of different training
configurations, all with the same architecture.  In particular, the
$x$ and $y$ axes are two important hyper-parameters for the neural
network (details to be explained later).  So we've trained $40 \times
40 = 1,600$ neural nets, for $40$ different choices of the first
hyper-parameter, and $40$ different choices of the second
hyper-parameter.  The intensity of colour at the corresponding point
on the graph represents how well the neural network performs: dark is
poor performance, light is good performance.  By examining a
judiciously selected set of these graphs we will deduce heuristic
rules for choosing hyper-parameters.

%
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=4in]{plots/example.eps}
\end{center}
\caption{ {\bf The performance of a single neural network architecture
    on a particular image recognition task}.  Lower cost represents
  better performance.  See the text for details.}
\label{fig:example}
\end{figure}

%XXX --- sigmoid, 20 hidden unit, friction versus learning rate.
%Labels: hyper-parameter 1.  hyper-parameter 2.  Cost: nice gradient
%bar thing.  Title(ish): After training epoch 3.



%
% expanding on the type of results we get
%
The last paragraph describes our basic approach.  In fact, we will
obtain not just single graphs, but entire animations showing how
neural networks improve as they are trained.  This lets us see in a
vivid way how the networks improve for different choices of
hyper-parameters.  In this journal we cannot embed the animations
directly, but will instead show sequences of graphs.  This is nearly
as good as the animations to convey the results.  The Supporting
Information contains the full animations.  We also develop several
other similar video-based visualization techniques, but this example
already conveys the general flavour.

%
% The problem with the conventional approach to heuristics paragraph
%
There's a kind of folk wisdom of artificial neural networks --- a
collection of commonly-used heuristics that experts use to set
hyper-parameters.  Such heuristics have been collected in volumes such
as~\cite{Montavon2012a}, as well as in many individual papers.  These
heuristics have led to many successes, and often really do improve
neural network performance.

%
% What can go wrong as a result
%
However, while such heuristics can be useful, they are often based on
the investigation of a small number of networks.  The problem with
using such small-scale anecdotal evidence is that people may come to
believe heuristics that are misleading or even wrong.  A heuristic may
hold for some particular examples, but not in other circumstances.
One can see this in the large number of papers which introduce a new
technique, and provide evidence that technique is superior for some
class of problems, but then other users of neural nets do not find any
benefits to the technique, outside extremely similar circumstances.

%
% example of where we can _see_ what's going wrong
%
Another problem is that even a heuristic which is literally true may
have significant caveats.  For example, there has been much recent
interest in rectified linear units (ReLUs, $max(0,x)$,
\cite{Glorot2010a,Nair2010a,Jarrett2009a}), a type of artificial
neuron which appears to learn faster than other common types of
neuron, such as sigmoid and tanh units. \cite{Krizhevsky2012a} Our
results confirm that this is often the case, but, as we shall see, it
is also often much harder to find hyper-parameters so ReLUs learn
quickly.  We are able to provide a compelling visualization
demonstrating this, and also to precisely quantify how much harder it
is to optimize with ReLUs.

%
% how we approach this problem
%
In our approach we plot animations of two-dimensional slices of
hyper-parameter space.  This lets us explicitly see how different
combinations of hyper-parameters affect performance.  Done
judiciously, this gives us a more comprehensive viewpoint from which
to derive heuristics, and more robust conclusions than in the
conventional approach described above.  Of course, these 2-d slices do
not provide us with a comprehensive point of view, because other
hyper-parameters are being held fixed.  Furthermore, for any given
slice we cannot guarantee that any heuristics deduced will hold for
other choices of hyper-parameter.  However, we can build confidence by
plotting slices for several different choices of the other
hyper-parameters.  This visualization-based approach seems to be new
and complementary to prior work.

%
% complementary to Bayesian parameter optimization
%
In recent years there has been considerable focus on automated
procedures for finding hyper-parameters.  The simplest approaches are
grid search XXX and variations.  More sophisticated methods are now
being developed, such as XXX, and Bayesian parameter optimization.
Encouraging recent results on Bayesian parameter optimization achieve
XXX.  However, to make Bayesian parameter optimization work well you
need good choices of prior distribution on the hyper-parameters.  The
results and approach we develop in this paper can help identify good
priors, and build confidence that a choice of priors is well motivated
(or, perhaps, should be modified).

%
% the curse of dimensionality: we're not claiming to escape it
%
Of course, our approach remains subject to the curse of
dimensionality.  Hyper-parameter space is high dimensional, and it's
inherently difficult to build up a comprehensive picture, even using
2-dimensional slices.  However, this statement of course also applies
to anyone trying to build up an understanding by hand-tuning single
hyper-parameters, trying to find just the ``right'' setting.  Our goal
is not to obtain a complete picture.  Instead, it's merely to develop
general (but partial) methods for understanding the structure of
hyper-parameter space.  It is a clear win to map out an entire 2-d
slice of hyper-parameter space at a fine-grained resolution, helping
us to better understand the relationships between those two
hyper-parameters.

% Results and Discussion can be combined.
\section*{Results and Discussion}

\subsection*{Experiments}

\subsubsection*{Learning rate versus momentum coefficient}

The learning algorithm we will use for our all experiments is
mini-batch stochastic gradient descent using momentum (XXX --- cite).
In our first set of experiments, described in this section, we
investigate learning as a function of two hyper-parameters: the
learning rate and the momentum coefficient.  Our experiments will lead
us to conjecture several heuristic relationships between these two
hyper-parameters.

Later sections will present similar experiments using other pairs of
hyper-parameters.  The next section gives a high-level overview of the
entire sequence of experiments, and discusses why different pairs of
hyper-parameters were chosen for investigation.  We could, of course,
have begun with that high-level overview.  However, since our
experiments are somewhat unusual in the neural networks literature, it
makes sense to start first with concrete results, since they will make
the high-level overview easier to understand.

Our experiments were performed using three neural network
architectures: a net with a single hidden layer containing 20 hidden
units; a net with a single hidden layer containing 40 hidden units;
and a net with two hidden layers, containing 40 and 20 hidden units,
respectively. All networks have an input layer with a neuron
corresponding to each pixel in the input image, in the case of the
grey-scale images in MNIST, and three neurons, corresponding to the
RGB intensities, for the images in CIFAR-10.  There is also an output
layer with a neuron for each classification label (ten for all the
datasets we use), followed by a softmax layer.  Weights were
initialized by sampling from a normal distribution with mean $0$ and
standard deviation $1/\sqrt{n}$, where $n$ is the number of inputs to
a neuron.  Biases were set to $0$.  Mini-batch size here and in later
experiments was set to $30$.

The experiments made use of four separate activation functions: the
sigmoid (i.e., logistic), hyperbolic tangent (tanh), rectified linear
units (ReLU), and softplus \cite{Glorot2010a,Dugas2001a}.  These
activation functions were applied throughout the network.

The experiments were performed using the MNIST (LeCun et al. (1998))
and CIFAR-10 (Krizhevsky (2009)) data sets.  These are well-known
labelled training sets used for image recognition.  Training was
performed using a negative log-likelihood cost function.

The graph in Figure~\ref{fig:mnist_basic} shows the behaviour of 1,600
neural networks when trained on the MNIST data for six epochs. These
correspond to 40 choices of the learning rate and 40 choices of the
momentum coefficient, for a total of 1,600 = 40 $\times$ 40 networks.

%
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=4in]{plots/detailed/LF-20R10R-20T10-MNIST-6.eps}
\end{center}
\caption{ {\bf The behaviour of 1,600 neural networks when trained on
    the MNIST data for six epochs.}}
\label{fig:mnist_basic}
\end{figure}
%
% XXX --- This is currently a lie, we ony have 400 = 20 x 20.
%
% XXX - check: what do people actually do in terms of exploring momentum?
% Take a a look at four or five papers.
%
% XXX - ideally, learning rate labels should be in factors of 10.
%
% XXX - we want to comment on the power law.  Replot with log momentum?
%
% XXX - Range: 10**-4 to 1?  SOme other range?

To generate this graph we sampled 40 different values for the momentum
coefficient, ranging uniformly over 0.0 to 1.0.  This sampling process
reflects the way many researchers informally explore to find suitable
values for the momentum coefficient, often trying a number of values,
0.0, 0.1, 0.2, 0.3, and then perhaps adjusting slightly once they've
found a good first approximation.

While we sampled uniformly for momentum, we instead sampled uniformly
on the logarithm of the learning rate.  This reflects the way many
researchers informally explore to find suitable values for the
learning rate, often trying a number of values, 0.1, 0.01, 0.001, and
so on.  A similar but more fine-grained approach is often used in
systematic grid searches.  We use a maximum value of 1.0 XXX for the
learning rate, since in all our MNIST experiments the nets did not
appear to be learning anything when the learning rate was that high.
We use a minimum value of 0.0001 XXX for the learning rate.  With
these choices our range for the learning rate includes values which
have been used in much of the prior literature on MNIST.

A natural question is why we vary the learning rate while holding the
number of epochs constant.  Naively, this seems unnatural, since the
distance traversed by the stochastic gradient descent algorithm will
be proportional to the learning rate times the number of training
epochs.

However, in practice, with fixed computational resources, the number
of epochs we can train for is roughly a constant.  And so the question
that confronts someone looking to choose good hyper-parameters is how
to choose the learning rate which gives the best results after a fixed
number of epochs.

There's an additional heuristic justification for this practice. When
searching for a local minimum, we first pass through a transient phase
in which the stochastic gradient descent algorithm tries to get near
that minimum as quickly as possible.  The higher the learning rate the
fewer epochs required.  But once that transient phase is over it is no
longer the speed of movement which matters.  If, in our examples, this
happens very early on --- say after one or fewer epochs --- then the
naive argument given above completely breaks down, and it makes sense
to study learning rate and momentum as independent variables, with the
total training time fixed.  We cannot defend this argument, since we
have not precisely defined transience, and thus cannot say whether we
exit this regime quickly in our examples.  However, we mention the
argument since some informal testing suggests that we do exit the
transient phase quickly over much of our hyper-parameter space.

% We don't actually know how to define transient, or what smoking gun
% to look for.  It might be interesting to look at a quantity like g_n
% . g_{n+1} / norms, where g_n is the gradient at the nth iteration.
% This probably starts out positive, and then transitions to zero at
% some point --- that's a good start on a definition of a transient
% phase.
%
% XXX --- OLD VERSION OF THE ABOVE ARGUMENT: The following is an
% alternative answer to the above question We don't have the evidence
% to support that we're in this post-transient phase (although we
% suspect we are for most of the parameters plotted)
%
% Wouldn't it make more sense to vary the number of epochs so the
% product with the learning rate was a constant?  This argument is
% correct during an initial transient phase in which we're trying to
% get into the neighbourhood of a reasonable solution to the cost
% optimization problem.  But it's no longer correct when exploring the
% microstructure of the fitness landscape near that neighborhood.

A weakness of the approach is that we only train for 6 epochs.  This
was chosen because it was computationally feasible given our computing
resources.  With more computing power it would be desirable to train
for more epochs.  We do note that the classification accuracies we
obtained after 6 epochs (using the best hyper-parameters) are
comparable to those obtained historically for similar architectures.
Note also that recent research XXX suggests the value of going to very
large numbers of epochs (on the order of $10^3$), and using an
annealing schedule for learning which ultimately results in very low
learning rates.  We do not investigate these possibilities here,
although they suggest natural questions for the future.

% XXX --- Some interesting speculation here.  We haven't actually done it.
% 
% Why did we go for n epochs (where n is whatever it is?)  Combination
% of two answers: (1) Here's our evidence that we're near asymptoting
% --- to obtain this, do curve fitting to estimate what the long-run
% cost will be, assume that the cost after 3 epochs is given by $c_1 +
% c_2 * exp(c_3 t)$, where t is the time; (2) Computationally, that's as
% much as we could do.  Shoot for n = 5 or 6 or so.  Note that we need
% to validate the use of the exponential as an approximation for the
% long-run cost.  We ignore first 3 epochs due to possible transients.

The following table summarizes our MNIST results by providing a graph,
like the previous one, for all architectures and all activation
functions. Each row corresponds to an activation function, and each
column to an architecture.  The architecture is indicated in the
column heading, e.g., the first column shows results with the 28
$\times$ 28 = 784 input units, 20 hidden units, and 10 output units.
The color scheme for cost and the axes are the same as in the previous
graph, but left unlabeled to avoid clutter.

\begin{tabular}{|c|c|c|c|}
        \hline
        & & Cost for the MNIST data & \\
        & & set after 6 epochs & \\
        \hline
         & 784-20-10 & 784-40-10 & 784-40-20-10\\ \hline
sigmoid 
        & \includegraphics[scale=0.25]{plots/simple/LF-20S10S-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40S10S-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40S20S10S-20T10-MNIST-6.eps} \\ \hline
tanh
        & \includegraphics[scale=0.25]{plots/simple/LF-20T10T-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40T10T-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40T20T10T-20T10-MNIST-6.eps} \\ \hline
ReLU 
        & \includegraphics[scale=0.25]{plots/simple/LF-20R10R-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40R10R-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40R20R10R-20T10-MNIST-6.eps} \\ \hline
XXX softplus 
        & \includegraphics[scale=0.25]{plots/simple/LF-20B10B-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40B10B-20T10-MNIST-6.eps}
        & \includegraphics[scale=0.25]{plots/simple/LF-40B20B10B-20T10-MNIST-6.eps} \\ \hline
\end{tabular}

As before, all these graphs visualize cost after 6 epochs of
training. Animated versions of these graphs, showing how cost changes
over training, are provided in the Supporting Information as Videos
S1--S12, along with the code that generates the videos.

We can summarize these results in another visualization, shown in
Figure~\label{fig:fraction}: (XXX - replot this without BReLu, fix
labels, do at epoch 6, use fraction not percent, change ``cost'' to
``cost threshold'') What this graph shows is the fraction of
hyper-parameters that achieve a cost below a given cost threshold,
plotted for several different activation functions.  So, for example,
let's look at the ReLU graph at a cost threshold of $0.05$.  We see
that the corresponding fraction is about $0.35$.  What this means is
that in our earlier ReLU experiments (fig.~XXX) about 35 percent of
the experiments achieved a cost below $0.05$.

\begin{figure}[!ht]
\begin{center}
% XXX \includegraphics[width=4in]{plots/detailed/LF-20a10a-20T10-MNIST-3-percent.eps}
\end{center}
\caption{ {\bf The fraction of hyper-parameters achieving a cost below
    a given cost threshold. Data shown are for MNIST.}}
\label{fig:fraction}
\end{figure}

We see that this graph quantifies not only how well a network does for
the best choice of hyper-parameters, but also how easy it is to tune.
We can see, for example, that ReLUs achieve slightly better best-case
performance than BReLUs (XXX--true for softplus?), but BReLUs are
significantly easier to tune to get near-to-best-case performance.

The above graph is static.  In Video S13 we show how the graph changes
as a function of training time.  For the particular case of MNIST and
the 20 hidden neuron architecture, the graph changes only a little
qualitatively over time.  Later, in our discussion of CIFAR-10, the
video will reveal striking qualitative changes in the performance of
the different activation functions.

The results above suggest a number of observations:

\textbf{Observation 1: softplus offers a good combination of
  performance and robustness:} For near-optimal costs, softplus (XXX)
has many more good hyper-parameters in the momentum-learning rate
space than there are for other activation functions.  For practical
purposes, softplus seems to be a pretty good choice.

\textbf{Observation 2: ReLUs have good performance, but suffer from
  unstable behaviour compared to softplus:} ReLUs have substantially
more stochastic behaviour.  They provide good best-case performance,
often comparable to softplus and better than sigmoid are tanh.  But
they are sensitive to small perturbations.  By contrast all our other
activation functions are much smoother.  This makes it easier to find
relatively stable solutions that won't just fall apart.

Informally, people who work with large, deep ReLU-based convolutional
neural networks do not report this kind of instability (cite
Goodfellow, personal communication XXX; c.f. also the results in arXiv
paper XXX).  However, so far as we know this has not been
quantitatively studied.  If the informal observation is borne out then
it would be interesting to understand the reasons for this difference.

Glorot \emph{et al}~\cite{Glorot2010a} have also done experiments
using ReLU and softplus in deep networks.  They found ReLU to
generally be better.  Although the networks they consider were very
different to ours, it would obviously be desirable to understand and
reconcile their results with ours.

\textbf{Observation 3: the momentum-learning rate tradeoff:} \emph{A
  priori}, the set of hyper-parameter values that result in the best
cost, or very near to the best cost, could take any number of shapes,
possibly quite complicated. For example, the near-optimal cost region
could be centred at a single point,
%
\begin{center}
\includegraphics[width=4in]{plots/detailed/OptimalPretend1.eps}
\end{center}
%
or perhaps in multiple disconnected regions,
%
\begin{center}
\includegraphics[width=4in]{plots/detailed/OptimalPretend2.eps}
\end{center}
%
We find, however, that the best results form a smooth, simple curve in
hyper-parameter space.  Surprisingly, the qualitative, log-like shape
of this curve is consistent across all examples, looking roughly like: 
%
\begin{center}
\includegraphics[width=4in]{plots/detailed/OptimalCorrect.eps}
\end{center}
%
This curve corresponds to the bottom of the valley-like cost functions
we saw above in Figures XXX.  This curve of near-optimal results can
be interpreted as a function mapping choices of momentum coefficient
to good choices of learning rate. Provided one uses this good choice
of learning rate, the cost resulting from training has little
dependence on the choice of momentum coefficient.  This is quite
remarkable, and means that for practical purposes setting momentum
coefficient to 0 and using regular gradient descent works about as
well as attempting to optimize the value of momentum.  These results
can be compared to those of Simard \emph{et al}~\cite{Simard2003a},
who also found little advantage in momentum-based approaches.
Parenthetically, we make the final remark that, inspecting the cost
functions of Figures~XXX, the cost appears to be a convex function of
learning rate.

%
% CIFAR:
%
\textbf{CIFAR-10:} Our results up to now have been for the MNIST data
set.  Let's switch to describing the analogous results for the
CIFAR-10 data set.  Note that the CIFAR-10 results were only obtained
for network architectures using 20 hidden units, due to the higher
computational cost of running networks for CIFAR-10.  The graph in
Figure~\ref{fig:cifar_basic} shows the behaviour of 1,600 ReLU-based
neural networks when trained on the CIFAR-10 data for six
epochs. These correspond to 40 choices of the learning rate and 40
choices of the momentum coefficient, for a total of 1,600 = 40
$\times$ 40 networks.  The momentum range is chosen in the same way as
for MNIST, uniformly over the range 0.0 to 1.0.  The learning rate is
chosen logarithmically, as for MNIST, but over a different range, from
0.1 to 0.000001, as this was better adapted to the CIFAR-10 data.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=4in]{plots/detailed/LF-20R10R-20T10-CIFAR-3.eps}
\end{center}
\caption{ {\bf The behaviour of 1,600 neural networks when trained on
    the CIFAR-10 data for six (XXX) epochs.} XXX - regenerated, get
  axes labels etc all consistent with CIFAR graph, and make sure that
  ranges are well chosen.}
\label{fig:cifar_basic}
\end{figure}

The following table (XXX) summarizes our CIFAR-10 results for all
activation functions.  Note that unlike MNIST there is now just a
single architecture.  The color scheme for cost and the axes are the
same as in the previous graph, but left unlabeled to avoid clutter.

\begin{tabular}{|c|c|}
        \hline
        & {\LARGE cost} \\
        & (CIFAR-10) \\
        & (negative log-likelihood) \\
        \hline
sigmoid 
        & \includegraphics[scale=0.25]{plots/simple/LF-20S10S-20T10-CIFAR-3.eps} \\ \hline
tanh 
        & \includegraphics[scale=0.25]{plots/simple/LF-20T10T-20T10-CIFAR-3.eps} \\ \hline
ReLU 
        & \includegraphics[scale=0.25]{plots/simple/LF-20R10R-20T10-CIFAR-3.eps} \\ \hline
BReLU XXX
        & \includegraphics[scale=0.25]{plots/simple/LF-20BR10BR-20T10-CIFAR-3.eps} \\ \hline
\end{tabular}

As for MNIST, we can summarize these results in the visualization
shown in Figure~\label{fig:fraction_CIFAR}: (XXX - replot this without
BReLu, fix labels, do at epoch 6, use fraction not percent, change
``cost'' to ``cost threshold'') As before, this graph shows the
fraction of hyper-parameters that achieve a cost below a given cost
threshold, plotted for several different activation functions.

\begin{figure}[!ht]
\begin{center}
%\includegraphics[width=4in]{plots/detailed/LF-20a10a-40T10-CIFAR-3-percent.eps}
\end{center}
\caption{ {\bf The fraction of hyper-parameters achieving a cost below
    a given cost threshold.  Data shown are for CIFAR-10.}}
\label{fig:fraction_CIFAR}
\end{figure}

XXX -- discuss results

As for MNIST, videos showing the time evolution of all the graphs
above are available in the Supporting Information, Videos S14--S17.

All the observations we made earlier for MNIST also hold for the
CIFAR-10 results.  However, this cannot be considered independent
confirmation of the validity of the observations, since the CIFAR-10
data was generated at the same time as the MNIST data, before we made
the observations.  However, it does suggest investigating these
observations more generally to determine the limits of their
applicability.

\subsubsection*{Further choices of hyper-parameter}

We've now looked in detail at how our networks behave as a function of
learning rate and momentum coefficient.  Of course, there are many
other hyper-parameters which can potentially be set.  Broadly
speaking, the hyper-parameters divide up into two types:
\begin{itemize}
\item \textbf{Learning parameters:} Including, for example, the
  momentum coefficient, learning rate, mini-batch size, training set
  (including training set size).

\item \textbf{Architectural parameters:} Structure of the layers,
  activation function, weight and bias initialization, cost function.
\end{itemize}
These are just a sample of the hyper-parameters.  In fact, many more
could easily be mentioned --- things like annealing schedule for
learning rate, per-layer (and perhaps even per-neuron) learning rates
and momentum coefficients, and so on.

Up to now we've been looking just at two hyper-parameters, learning
rate and momentum coefficient.  Later on we'll conduct similar
experiments generating two-dimensional plots based on the following
pairs of hyper-parameters: (1) weight initialization in layer 1 versus
layer 2; (2) learning rate in layer 1 versus layer 2; (3) number of
hidden units versus size of training data.

In each case the visualizations we obtain will lead us to some
striking observations, and hypotheses which may perhaps be confirmed
by future experiments.

One may naturally ask why we make these particular choices of 2d
section?  There are many other possibilities that are clearly worth
investigating.  Our choices in this paper arose while working on
another project in which these particular questions\footnote{Except
  (3), the number of hidden units versus size of training data, which
  was done entirely for curiosity's sake.} arose as a practical matter
affecting our choice of hyper-parameters. E.g., we needed to make
choices regarding the learning rate / momentum coefficient tradeoff.
Of course a thorough investigation of all possible pairs of
hyper-parameters would be a gargantuan undertaking, absorbing a great
deal of time (and computational power).  So to some extent our choices
were arbitrary, but they were motivated by practical concerns, and we
believe the visualizations obtained have considerable independent
interest.

A possible quibble with our approach is that we have focused on
plotting cost, not classification accuracy.  We chose to focus on cost
because fundamentally this is what the stochastic gradient descent
algorithm is trying to minimize.  Of course, for practical purposes
classification accuracy of course matters, as well as issues such as
overfitting and choice of cost function.  Further work could be done
to address these issues.  The code we've made available (XXX) could,
of course, be adapted to study these issues.

\subsubsection*{Weight initialization in layer 1 versus layer 2}

Let us now move away from learning rate and momentum, and consider
instead other hyper-parameters.  One crucial class of hyper-parameters
in a neural network is the way weights in the network are initialized.
With poor initialization, networks can completely fail to learn.  By
contrast, much of the recent progress in deep learning has come from
finding better initialization techniques.  To gain some insight into
impact of weight initialization we conducted some simple experiments
to visualize the impact of different weight initialization choices.

We considered the CIFAR-10 classification problem.  We used a single
neural network architecture, with $32 \times 32 \times 3$ input
neurons, encoding the different pixel channel intensities, a single
hidden layer containing 20 hidden units, and 10 output neurons, with a
softmax.  We considered three (XXX) different activation functions:
sigmoid, tanh, and ReLU.  All biases are initialized to $0$, and
learning rate and momentum coefficients selected using the optimal
values from the experiments in the previous section.

In this network, choices need to be made about how to initialize the
weights.  A common strategy in a fully-connected network like this is
that a neuron with $n$ inputs has weights sampled from a normal
distribution with mean $0$ and standard deviation $1/\sqrt{n}$.  In
our experiments we used two parameters, which we shall $w_1$ and
$w_2$.  Weights in the first layer are initialized using normal
distributions with mean $0$ and standard deviation $w_1$.  Weights in
the second layer are initialized using normal distributions with mean
$0$ and standard deviation $w_2$.  We allow both of these parameters
to vary from $0.0$ to $0.5$, and take $20$ parameter values for $w_1$
uniformly space in this range, and similarly for $w_2$.  The results
are illustrated in the table below:

XXX: sigmoid, tanh, ReLU.  Add special points showing various popular
schemes: 1/ rt n, 1/ rt n+m (Bengio paper on hyper-parameters).  Look
for other popular approaches?

Videos showing the time evolution of these graphs are available in the
Supporting Information, Videos S18--S20.

\textbf{Observation 4: The sigmoid results are almost insensitive to
  initialization.}  More precisely, provided the cost does not diverge
it seems to matter little how the weight initalizations are chosen.
We see a similar, albeit weaker, effect for the tanh activation.

\textbf{Obervation 5: The ReLU results depend sensitively on
  initialization, and show a lot of stochastic variance.}  We see a
weak effect whereby a large value for $w_1$ can to some extent be
compensated by a small value for $w_2$ and vice versa.  Looking along
hyperbolae in the graphs we can see that, very roughly, it is the
product $w_1 w_2$ that determines the behaviour.

\subsubsection*{Learning rate in layer 1 versus layer 2}

Architecture: 20 HU, 40 HU, 40+20 HU (fixed learning rate in final
layer).

Activation functions: sigmoid, ReLU, tanh, BReLU.

Training data: CIFAR.

Claim: Best rates not on diagonal.

\subsubsection*{Number of hidden units versus size of training data}

4. Number of HU (single layer) versus size of training data

Architecture: single layer, + softmax.

Activation: sigmoid.

Training data: MNIST.


CLaim: Surprising how little data, or how littl HU are required.

\subsection*{Conclusion}

OBSERVATION / RELATION TO PRIOR WORK: Over the past few years there's
been a lot of work using GPUs [refs] and cluster s[ refs] to train
neural networks.  Most of this work has focused on training larger and
deeper networks, for more epochs than before, to get superior
performance.  Our work demonstrates a complementary strategy: instead
of using the increased computational power to train one or a few very
large networks, we use it to train very large numbers of relatively
simple networks.  We believe this dual strategy can be important to

+ LImit of the technique: These are only preliminary results.  Really
you want a much more comprehensive investigation.  It is even
possible, a priori, that there are no simple, esasily visualizable
rules of thumb to discover!  However, we believe it is worth at least
investigating the possibility, and our preliminary results suggest
that there is a surprisingly simple struture underlying regions of
hyperparameter space

+ Fun idea: AN atlas / periodic table of neural nets.  Would contain
lots of heuristics, and lots of failed heuristics that seem plausile
(but with disproofs).  Supprting evidence: lots and lots of videos.
Hundreds / thousands.

+ Sample the space of hyper-parameters, then try to learn the manifold
of optimal hyper-parameters.

+ THe lower dimensional manifold: the parameters which are optimal in
hyperparameter space.


(Note: In some sense, it's 2+1 dimension, because we're actually
looking over training time.  We do Not look to saturate, i.e., to
get to optimal performance.  So the results are more about how quickly
we're learning, not asymptotic capacity.  Add a caveat: in fact,
almost noone ever does this.  One of the lessons of recent work is
that there's much more benefit to going to long training times than
people previously thought, with a suitable schedule of annealing.)


% NOTES FOR FUTURE PROBLEMS
%
% Nesterov.  Annealing.  Adaptive hyper-parameters.


+ ??? To talk about: low-dimensional ``optimal'' manifold in
hyperparameter space.  Bengio's ICML2011 for algorithms on tuning
hyperparameters.

+ Obviously 3d versions... hard to computationally.


NOTES FOR THIS PAPER

XXX --- a general note: what the hell are we actually doing?  These
are NOT experiments to confirm particular hypotheses.  The point of
the paper is to demonstrate a TECHNIQUE by which hypotheses may be
generated, and also by which hypotheses may be ruled out.  Of course,
the technique could be applied to test and establish such hypotheses,
although it would require substantially more computing power than we
presently have access too.  In the meantime, the results from
application of the technique seem sufficiently striking as to make
them worth knowing widely.

XXX --- We need to make the last point really strongly.  It should be
in the intro, and perhaps reiterated a couple of times later in the
paper.  The last para should be inserted somewehre in the intro.


Q: Should we introduce some notion of local variance / local
dependence on variation of parameters?  Chris: Hold the initialization
and ordering of data constant, and try the 40 x 4 experiments for a
ReLU subexperiment --- do we see the same ``splotchiness''?

% Do NOT remove this, even if you are not including acknowledgments
\section*{Acknowledgments}

Thanks to David Dalrymple and Jen Dodd for helpful discussions.

%\section*{References}
% The bibtex filename
\bibliography{cifar}

\section*{Figure Legends}
%\begin{figure}[!ht]
%\begin{center}
%%\includegraphics[width=4in]{figure_name.2.eps}
%\end{center}
%\caption{
%{\bf Bold the first sentence.}  Rest of figure 2  caption.  Caption 
%should be left justified, as specified by the options to the caption 
%package.
%}
%\label{Figure_label}
%\end{figure}


\section*{Tables}
%\begin{table}[!ht]
%\caption{
%\bf{Table title}}
%\begin{tabular}{|c|c|c|}
%table information
%\end{tabular}
%\begin{flushleft}Table caption
%\end{flushleft}
%\label{tab:label}
% \end{table}

\end{document}
